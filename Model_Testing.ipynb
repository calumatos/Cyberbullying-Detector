{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ae2d855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d1d896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the functions:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import functions\n",
    "import re\n",
    "import nltk\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39dd90fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = tf.keras.models.load_model('tf2_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61dcaaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = joblib.load('saved_tokenizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d90442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model\n",
    "word2vec_model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00bd0a4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 1000)         144008000 \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 500000)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                32000064  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 176008129 (671.42 MB)\n",
      "Trainable params: 32000129 (122.07 MB)\n",
      "Non-trainable params: 144008000 (549.35 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81a16a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.preprocessing.text.Tokenizer at 0x20479994390>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57fd19d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[33224], [28324], [22641], [22641], [27210]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "742737db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_message():\n",
    "    # Get input from the user\n",
    "    message = input(\"Enter the text you want to classify: \")\n",
    "    \n",
    "    # Preprocess the input message\n",
    "    cleaned_message = functions.clean_text(message)\n",
    "    print(\"Cleaned message:\", cleaned_message)\n",
    "    tokenized_message = functions.tokenizing_and_stopwords(cleaned_message)\n",
    "    print(\"Tokenized message:\", tokenized_message)\n",
    "    lemmatized_message = functions.lemmatization_and_stopwords(tokenized_message)\n",
    "    print(\"Lemmatized message:\", lemmatized_message)\n",
    "    \n",
    "    # Convert text to sequence\n",
    "    sequence = tokenizer.texts_to_sequences(lemmatized_message)\n",
    "    print(\"Tokenized sequence:\", sequence)\n",
    "    \n",
    "    # Pad sequence\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=500) \n",
    "    print(\"Padded sequence:\", padded_sequence)\n",
    "    \n",
    "    # Make predictions using the loaded model\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    print(prediction)\n",
    "    \n",
    "    if prediction[0] > 0.5:\n",
    "        print(\"\\n--- THIS IS BULLYING!!! --- This message is classified as bullying.\")\n",
    "    else:\n",
    "        print(\"\\n--- NO BULLYING. --- This message is not classified as bullying.\")  \n",
    "    \n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6c3c483",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the text you want to classify: you bitch\n",
      "Cleaned message: you bitch\n",
      "Tokenized message: bitch\n",
      "Lemmatized message: bitch\n",
      "Tokenized sequence: [[17154], [9754], [16234], [12887], [33224]]\n",
      "Padded sequence: [[    0     0     0 ...     0     0 17154]\n",
      " [    0     0     0 ...     0     0  9754]\n",
      " [    0     0     0 ...     0     0 16234]\n",
      " [    0     0     0 ...     0     0 12887]\n",
      " [    0     0     0 ...     0     0 33224]]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "[[8.3923078e-01]\n",
      " [6.7725894e-03]\n",
      " [8.4583871e-06]\n",
      " [6.5330166e-01]\n",
      " [3.4280559e-03]]\n",
      "\n",
      "--- THIS IS BULLYING!!! --- This message is classified as bullying.\n"
     ]
    }
   ],
   "source": [
    "prediction = classify_message()\n",
    "\n",
    "# Thanks to all of you for the amazing time. We wish all of you \n",
    "# a glorious future from the bottom of ours hearts. <3 Love, Carmen & Juli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db845ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee336e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef7c02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the second model\n",
    "class_model = tf.keras.models.load_model('tf3_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fff286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "class_tokenizer = joblib.load('class_tokenizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model\n",
    "#class_word2vec_model = Word2Vec.load(\"class_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "990d202d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 500)          15803000  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               240400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                6464      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16050189 (61.23 MB)\n",
      "Trainable params: 16050189 (61.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d8c9b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.preprocessing.text.Tokenizer at 0x1b96b6d0a90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a31ccc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### testing with the classification type\n",
    "\n",
    "def classify_message():\n",
    "    # Get input from the user\n",
    "    message = input(\"Enter the text you want to classify: \")\n",
    "    \n",
    "    # Preprocess the input message\n",
    "    cleaned_message = functions.clean_text(message)\n",
    "    print(\"Cleaned message:\", cleaned_message)\n",
    "    tokenized_message = functions.tokenizing_and_stopwords(cleaned_message)\n",
    "    print(\"Tokenized message:\", tokenized_message)\n",
    "    lemmatized_message = functions.lemmatization_and_stopwords(tokenized_message)\n",
    "    print(\"Lemmatized message:\", lemmatized_message)\n",
    "    \n",
    "    # Convert text to sequence\n",
    "    sequence = tokenizer.texts_to_sequences(lemmatized_message)\n",
    "    #print(\"Tokenized sequence:\", sequence)\n",
    "    \n",
    "    # Pad sequence\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=500) \n",
    "    #print(\"Padded sequence:\", padded_sequence)\n",
    "    \n",
    "    # Make predictions using the loaded model\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    print(prediction)\n",
    "      \n",
    "    if prediction[0] > 0.5:\n",
    "        print(\"\\n--- THIS IS BULLYING!!! --- This message is classified as bullying.\")\n",
    "        \n",
    "        # Use the second model to classify the type of bullying\n",
    "        class_sequence = class_tokenizer.texts_to_sequences(lemmatized_message)\n",
    "        class_padded_sequence = pad_sequences(class_sequence, maxlen=500) \n",
    "        class_prediction = class_model.predict(class_padded_sequence)\n",
    "        print(\"\\nType of Bullying:\", class_prediction)\n",
    "\n",
    "    else:\n",
    "        print(\"\\n--- NO BULLYING. --- This message is not classified as bullying.\")  \n",
    "    \n",
    "    return class_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70ba7c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the text you want to classify: you bitch\n",
      "Cleaned message: you bitch\n",
      "Tokenized message: bitch\n",
      "Lemmatized message: bitch\n",
      "1/1 [==============================] - 0s 423ms/step\n",
      "[[8.3923078e-01]\n",
      " [6.7725894e-03]\n",
      " [8.4583871e-06]\n",
      " [6.5330166e-01]\n",
      " [3.4280559e-03]]\n",
      "\n",
      "--- THIS IS BULLYING!!! --- This message is classified as bullying.\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "\n",
      "Type of Bullying: [[1.0000000e+00 1.0671426e-09 2.9529843e-09 2.6727087e-09 8.3026752e-10]\n",
      " [1.0000000e+00 1.0671426e-09 2.9529843e-09 2.6727087e-09 8.3026752e-10]\n",
      " [1.0000000e+00 1.0671426e-09 2.9529843e-09 2.6727087e-09 8.3026752e-10]\n",
      " [1.0000000e+00 1.0671426e-09 2.9529843e-09 2.6727087e-09 8.3026752e-10]\n",
      " [1.0000000e+00 1.0671426e-09 2.9529843e-09 2.6727087e-09 8.3026752e-10]]\n"
     ]
    }
   ],
   "source": [
    "class_prediction = classify_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe13d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
