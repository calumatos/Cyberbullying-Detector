{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b47f014",
   "metadata": {},
   "source": [
    "We have 8 csv files:\n",
    "\n",
    "- Bullying_1: aggression_parsed_dataset.csv       (115864 rows × 5 columns)\n",
    "- Bullying_2: attack_parsed_dataset.csv           (115864 rows × 5 columns) \n",
    "- Bullying_3: twitter_parsed_dataset.csv          (16851 rows × 5 columns) \n",
    "- Bullying_4: twitter_racism_parsed_dataset.csv   (13471 rows × 5 columns) \n",
    "- Bullying_5: twitter_sexism_parsed_dataset.csv   (14881 rows × 5 columns) \n",
    "- Bullying_6: youtube_parsed_dataset.csv          (3464 rows × 10 columns) \n",
    "- Bullying_7: toxicity_parsed_dataset.csv         (159686 rows × 5 columns) \n",
    "- Bullying_8: kaggle_parsed_dataset.csv           (8799 rows × 4 columns) \n",
    "\n",
    "\n",
    "https://www.kaggle.com/datasets/saurabhshahane/cyberbullying-dataset?select=aggression_parsed_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d029048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import multilabel_confusion_matrix, hamming_loss, classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9af3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "B_types = pd.read_csv(\"B_types.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5629addb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>type_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fuck you bitch. and fuck you lousy and misle...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aggresion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @ryanklang: @Norse_Gamer @MT8_9 @ActionFlic...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>::Hopefully one day the editors of Wikipedia ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>toxicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>::::What a deceptive little creature you are...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aggresion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lol smalljim.  u have small penis. jk lol</td>\n",
       "      <td>1.0</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35195</th>\n",
       "      <td>I FUCKING HATE NIGGERS AND SPICS AND JEWS AND...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>toxicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35196</th>\n",
       "      <td>`  == Congratulations ==  Well I suppose I hav...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aggresion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35197</th>\n",
       "      <td>== You too! ==  You're also gonna get it, di...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35198</th>\n",
       "      <td>Stop fucking harrassing me!</td>\n",
       "      <td>1.0</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35199</th>\n",
       "      <td>I LOVE SUCKING COCK!    GIVE ME CUM OR GIVE ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  oh_label type_label\n",
       "0        fuck you bitch. and fuck you lousy and misle...       1.0  aggresion\n",
       "1      RT @ryanklang: @Norse_Gamer @MT8_9 @ActionFlic...       1.0     sexism\n",
       "2       ::Hopefully one day the editors of Wikipedia ...       1.0   toxicity\n",
       "3        ::::What a deceptive little creature you are...       1.0  aggresion\n",
       "4              lol smalljim.  u have small penis. jk lol       1.0     attack\n",
       "...                                                  ...       ...        ...\n",
       "35195   I FUCKING HATE NIGGERS AND SPICS AND JEWS AND...       1.0   toxicity\n",
       "35196  `  == Congratulations ==  Well I suppose I hav...       1.0  aggresion\n",
       "35197    == You too! ==  You're also gonna get it, di...       1.0     attack\n",
       "35198                        Stop fucking harrassing me!       1.0     attack\n",
       "35199    I LOVE SUCKING COCK!    GIVE ME CUM OR GIVE ...       1.0     attack\n",
       "\n",
       "[35200 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original dataframe\n",
    "B_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972d8640",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_types['type_label'] = B_types['type_label'].replace('aggresion', 'aggression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b465fbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_label\n",
       "aggression    10000\n",
       "toxicity      10000\n",
       "attack        10000\n",
       "sexism         3300\n",
       "racism         1900\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_types['type_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "524c5bdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>type_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12828</th>\n",
       "      <td>I swear, you're extremely stupid and oblivious...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aggression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  oh_label  type_label\n",
       "12828  I swear, you're extremely stupid and oblivious...       1.0  aggression"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample of aggression\n",
    "B_types.loc[B_types[\"type_label\"]=='aggression'].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02df6463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>type_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21421</th>\n",
       "      <td>I have a Ph.D so I think I am better than ever...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  oh_label type_label\n",
       "21421  I have a Ph.D so I think I am better than ever...       1.0     attack"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample of attack\n",
    "B_types.loc[B_types[\"type_label\"]=='attack'].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c3d168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>type_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23013</th>\n",
       "      <td>@Lithobolos @ZaibatsuNews I don't care what th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  oh_label type_label\n",
       "23013  @Lithobolos @ZaibatsuNews I don't care what th...       1.0     racism"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample of racism\n",
    "B_types.loc[B_types[\"type_label\"]=='racism'].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b92615b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>type_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16662</th>\n",
       "      <td>RT @AlexNew93 I'm not sexist but I just cannot...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  oh_label type_label\n",
       "16662  RT @AlexNew93 I'm not sexist but I just cannot...       1.0     sexism"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample of sexism\n",
    "B_types.loc[B_types[\"type_label\"]=='sexism'].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f600605e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>type_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27122</th>\n",
       "      <td>`  == Stop your fucking spamming ==  I have to...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>toxicity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  oh_label type_label\n",
       "27122  `  == Stop your fucking spamming ==  I have to...       1.0   toxicity"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample of toxicity\n",
    "B_types.loc[B_types[\"type_label\"]=='toxicity'].sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7207e",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0faa7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text Preprocessing:\n",
    "# Cleaning and preprocessing the text data. This involves tasks like removing stop words, handling emojis, \n",
    "# and converting text to lowercase.\n",
    "# Tokenizing the text to convert it into a format suitable for machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80d79c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text          0\n",
       "oh_label      0\n",
       "type_label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking null values\n",
    "B_types.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b426bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the text. Cenverting to lower_case, removing any special characters/symbols and removing extra spaces.\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text) # Remove special characters and symbols\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.MULTILINE) # Remove extra spaces\n",
    "    words = [word for word in text.split() if word.isalnum() and word.isalpha()] # Split the text into words and keep only alphanumeric words\n",
    "    cleaned_text = ' '.join(words) # Join the words with a space\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb50a941",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fuck you bitch and fuck you lousy and misleadi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt ryanklang actionflickdoc hey look me too ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hopefully one day the editors of wikipedia wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what a deceptive little creature you are indee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lol smalljim u have small penis jk lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35195</th>\n",
       "      <td>i fucking hate niggers and spics and jews and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35196</th>\n",
       "      <td>congratulations well i suppose i have to confe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35197</th>\n",
       "      <td>you too you re also gonna get it dickface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35198</th>\n",
       "      <td>stop fucking harrassing me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35199</th>\n",
       "      <td>i love sucking cock give me cum or give me death</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Cleaned_Text\n",
       "0      fuck you bitch and fuck you lousy and misleadi...\n",
       "1      rt ryanklang actionflickdoc hey look me too ht...\n",
       "2      hopefully one day the editors of wikipedia wil...\n",
       "3      what a deceptive little creature you are indee...\n",
       "4                 lol smalljim u have small penis jk lol\n",
       "...                                                  ...\n",
       "35195  i fucking hate niggers and spics and jews and ...\n",
       "35196  congratulations well i suppose i have to confe...\n",
       "35197          you too you re also gonna get it dickface\n",
       "35198                         stop fucking harrassing me\n",
       "35199   i love sucking cock give me cum or give me death\n",
       "\n",
       "[35200 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the clean_text function to the \"Text\" column\n",
    "B_types['Cleaned_Text'] = B_types['Text'].apply(clean_text)\n",
    "B_types[['Cleaned_Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37ee5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.data.path.append('/root/nltk_data/corpora/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35b5b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing and removing stopwords\n",
    "\n",
    "def tokenizing_and_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    text = \" \".join(filtered_tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c76860f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ctns_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fuck bitch fuck lousy misleading wikipedia che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt ryanklang actionflickdoc hey look http co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hopefully one day editors wikipedia wake turf ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deceptive little creature indeed assuming good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lol smalljim u small penis jk lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35195</th>\n",
       "      <td>fucking hate niggers spics jews minorities fuc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35196</th>\n",
       "      <td>congratulations well suppose confess wrong poi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35197</th>\n",
       "      <td>also gon na get dickface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35198</th>\n",
       "      <td>stop fucking harrassing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35199</th>\n",
       "      <td>love sucking cock give cum give death</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               ctns_Text\n",
       "0      fuck bitch fuck lousy misleading wikipedia che...\n",
       "1           rt ryanklang actionflickdoc hey look http co\n",
       "2      hopefully one day editors wikipedia wake turf ...\n",
       "3      deceptive little creature indeed assuming good...\n",
       "4                      lol smalljim u small penis jk lol\n",
       "...                                                  ...\n",
       "35195  fucking hate niggers spics jews minorities fuc...\n",
       "35196  congratulations well suppose confess wrong poi...\n",
       "35197                           also gon na get dickface\n",
       "35198                            stop fucking harrassing\n",
       "35199              love sucking cock give cum give death\n",
       "\n",
       "[35200 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the tokenizing_and_stopwords function to the \"Cleaned_Text\" column\n",
    "B_types['ctns_Text'] = B_types['Cleaned_Text'].apply(tokenizing_and_stopwords)  \n",
    "B_types[['ctns_Text']] #ctns_Text = cleaned, tokenized, no stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46839502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ian black caley seen ian black play irractic life endagering challenges dunfermline rangers players show actual fucking nutter played blackburn youth team still'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_types['ctns_Text'][94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f1ff1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization aims to reduce words to their base or root form.\n",
    "\n",
    "def lemmatization_and_stopwords(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    clean_text = []\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    for token in tokens:\n",
    "        if token.lower() not in stopwords.words('english') and len(token) > 3:\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "            clean_text.append(token)\n",
    "    result_text = \" \".join(clean_text)\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d334a90d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fuck bitch fuck lousy misleading wikipedia che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ryanklang actionflickdoc look http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hopefully editor wikipedia wake turf rubbish l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deceptive little creature indeed assuming good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>smalljim small penis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35195</th>\n",
       "      <td>fucking hate nigger spic jew minority fucking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35196</th>\n",
       "      <td>congratulation well suppose confess wrong poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35197</th>\n",
       "      <td>also dickface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35198</th>\n",
       "      <td>stop fucking harrassing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35199</th>\n",
       "      <td>love sucking cock give give death</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lemm_text\n",
       "0      fuck bitch fuck lousy misleading wikipedia che...\n",
       "1                     ryanklang actionflickdoc look http\n",
       "2      hopefully editor wikipedia wake turf rubbish l...\n",
       "3      deceptive little creature indeed assuming good...\n",
       "4                                   smalljim small penis\n",
       "...                                                  ...\n",
       "35195  fucking hate nigger spic jew minority fucking ...\n",
       "35196  congratulation well suppose confess wrong poin...\n",
       "35197                                      also dickface\n",
       "35198                            stop fucking harrassing\n",
       "35199                  love sucking cock give give death\n",
       "\n",
       "[35200 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply lemmatization function to the \"ctns_Text\" column\n",
    "B_types['lemm_text'] = B_types['ctns_Text'].apply(lemmatization_and_stopwords)\n",
    "B_types[['lemm_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36f1dc59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>type_label</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>ctns_Text</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fuck you bitch. and fuck you lousy and misle...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aggression</td>\n",
       "      <td>fuck you bitch and fuck you lousy and misleadi...</td>\n",
       "      <td>fuck bitch fuck lousy misleading wikipedia che...</td>\n",
       "      <td>fuck bitch fuck lousy misleading wikipedia che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @ryanklang: @Norse_Gamer @MT8_9 @ActionFlic...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sexism</td>\n",
       "      <td>rt ryanklang actionflickdoc hey look me too ht...</td>\n",
       "      <td>rt ryanklang actionflickdoc hey look http co</td>\n",
       "      <td>ryanklang actionflickdoc look http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>::Hopefully one day the editors of Wikipedia ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>toxicity</td>\n",
       "      <td>hopefully one day the editors of wikipedia wil...</td>\n",
       "      <td>hopefully one day editors wikipedia wake turf ...</td>\n",
       "      <td>hopefully editor wikipedia wake turf rubbish l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>::::What a deceptive little creature you are...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aggression</td>\n",
       "      <td>what a deceptive little creature you are indee...</td>\n",
       "      <td>deceptive little creature indeed assuming good...</td>\n",
       "      <td>deceptive little creature indeed assuming good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lol smalljim.  u have small penis. jk lol</td>\n",
       "      <td>1.0</td>\n",
       "      <td>attack</td>\n",
       "      <td>lol smalljim u have small penis jk lol</td>\n",
       "      <td>lol smalljim u small penis jk lol</td>\n",
       "      <td>smalljim small penis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35195</th>\n",
       "      <td>I FUCKING HATE NIGGERS AND SPICS AND JEWS AND...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>toxicity</td>\n",
       "      <td>i fucking hate niggers and spics and jews and ...</td>\n",
       "      <td>fucking hate niggers spics jews minorities fuc...</td>\n",
       "      <td>fucking hate nigger spic jew minority fucking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35196</th>\n",
       "      <td>`  == Congratulations ==  Well I suppose I hav...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aggression</td>\n",
       "      <td>congratulations well i suppose i have to confe...</td>\n",
       "      <td>congratulations well suppose confess wrong poi...</td>\n",
       "      <td>congratulation well suppose confess wrong poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35197</th>\n",
       "      <td>== You too! ==  You're also gonna get it, di...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>attack</td>\n",
       "      <td>you too you re also gonna get it dickface</td>\n",
       "      <td>also gon na get dickface</td>\n",
       "      <td>also dickface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35198</th>\n",
       "      <td>Stop fucking harrassing me!</td>\n",
       "      <td>1.0</td>\n",
       "      <td>attack</td>\n",
       "      <td>stop fucking harrassing me</td>\n",
       "      <td>stop fucking harrassing</td>\n",
       "      <td>stop fucking harrassing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35199</th>\n",
       "      <td>I LOVE SUCKING COCK!    GIVE ME CUM OR GIVE ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>attack</td>\n",
       "      <td>i love sucking cock give me cum or give me death</td>\n",
       "      <td>love sucking cock give cum give death</td>\n",
       "      <td>love sucking cock give give death</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  oh_label  \\\n",
       "0        fuck you bitch. and fuck you lousy and misle...       1.0   \n",
       "1      RT @ryanklang: @Norse_Gamer @MT8_9 @ActionFlic...       1.0   \n",
       "2       ::Hopefully one day the editors of Wikipedia ...       1.0   \n",
       "3        ::::What a deceptive little creature you are...       1.0   \n",
       "4              lol smalljim.  u have small penis. jk lol       1.0   \n",
       "...                                                  ...       ...   \n",
       "35195   I FUCKING HATE NIGGERS AND SPICS AND JEWS AND...       1.0   \n",
       "35196  `  == Congratulations ==  Well I suppose I hav...       1.0   \n",
       "35197    == You too! ==  You're also gonna get it, di...       1.0   \n",
       "35198                        Stop fucking harrassing me!       1.0   \n",
       "35199    I LOVE SUCKING COCK!    GIVE ME CUM OR GIVE ...       1.0   \n",
       "\n",
       "       type_label                                       Cleaned_Text  \\\n",
       "0      aggression  fuck you bitch and fuck you lousy and misleadi...   \n",
       "1          sexism  rt ryanklang actionflickdoc hey look me too ht...   \n",
       "2        toxicity  hopefully one day the editors of wikipedia wil...   \n",
       "3      aggression  what a deceptive little creature you are indee...   \n",
       "4          attack             lol smalljim u have small penis jk lol   \n",
       "...           ...                                                ...   \n",
       "35195    toxicity  i fucking hate niggers and spics and jews and ...   \n",
       "35196  aggression  congratulations well i suppose i have to confe...   \n",
       "35197      attack          you too you re also gonna get it dickface   \n",
       "35198      attack                         stop fucking harrassing me   \n",
       "35199      attack   i love sucking cock give me cum or give me death   \n",
       "\n",
       "                                               ctns_Text  \\\n",
       "0      fuck bitch fuck lousy misleading wikipedia che...   \n",
       "1           rt ryanklang actionflickdoc hey look http co   \n",
       "2      hopefully one day editors wikipedia wake turf ...   \n",
       "3      deceptive little creature indeed assuming good...   \n",
       "4                      lol smalljim u small penis jk lol   \n",
       "...                                                  ...   \n",
       "35195  fucking hate niggers spics jews minorities fuc...   \n",
       "35196  congratulations well suppose confess wrong poi...   \n",
       "35197                           also gon na get dickface   \n",
       "35198                            stop fucking harrassing   \n",
       "35199              love sucking cock give cum give death   \n",
       "\n",
       "                                               lemm_text  \n",
       "0      fuck bitch fuck lousy misleading wikipedia che...  \n",
       "1                     ryanklang actionflickdoc look http  \n",
       "2      hopefully editor wikipedia wake turf rubbish l...  \n",
       "3      deceptive little creature indeed assuming good...  \n",
       "4                                   smalljim small penis  \n",
       "...                                                  ...  \n",
       "35195  fucking hate nigger spic jew minority fucking ...  \n",
       "35196  congratulation well suppose confess wrong poin...  \n",
       "35197                                      also dickface  \n",
       "35198                            stop fucking harrassing  \n",
       "35199                  love sucking cock give give death  \n",
       "\n",
       "[35200 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To show all the 'processed text' columns\n",
    "B_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33537dbc",
   "metadata": {},
   "source": [
    "### Splitting the Dataset - X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66f8f76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 28160 28160\n",
      "Testing set length: 7040 7040\n"
     ]
    }
   ],
   "source": [
    "X = B_types['lemm_text']\n",
    "y = B_types[['type_label']] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the lengths of the resulting sets\n",
    "print(\"Training set length:\", len(X_train), len(y_train))\n",
    "print(\"Testing set length:\", len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e3b475",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4750676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Extraction: Extracting relevant features from the text data. \n",
    "# Using word embeddingsb (Word2Vec, GloVe) to capture semantic relationships between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a445e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = B_types['lemm_text']\n",
    "model = Word2Vec(sentences, vector_size=500, window=5, min_count=1, workers=4)\n",
    "#model.save(\"class_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1b21410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (28160, 500) (28160, 1)\n",
      "Testing set shape: (7040, 500) (7040, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train.fillna(\"\", inplace=True)\n",
    "X_test.fillna(\"\", inplace=True)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to have a consistent length\n",
    "max_sequence_length = 500  # You can adjust this based on your data\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "print(\"Training set shape:\", X_train_padded.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test_padded.shape, y_test.shape)\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "word2vec_model = Word2Vec.load(\"class_word2vec.model\")\n",
    "\n",
    "# Create an embedding matrix using Word2Vec\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, word2vec_model.vector_size))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "# Save the tokenizer\n",
    "#joblib.dump(tokenizer, 'class_tokenizer.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "349d1b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aggression', 'sexism', 'toxicity', 'attack', 'racism'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_types['type_label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123fdc86",
   "metadata": {},
   "source": [
    "#### TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8efccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string labels to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert numerical labels to one-hot encoded vectors\n",
    "y_train_one_hot = to_categorical(y_train_encoded, num_classes=5)\n",
    "y_test_one_hot = to_categorical(y_test_encoded, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81f31f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, \n",
    "                    output_dim=word2vec_model.vector_size, \n",
    "                    input_length=max_sequence_length))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46e95e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b36c5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "880/880 [==============================] - 946s 1s/step - loss: 1.1005 - accuracy: 0.3868 - val_loss: 1.0164 - val_accuracy: 0.4128\n",
      "Epoch 2/3\n",
      "880/880 [==============================] - 1109s 1s/step - loss: 0.9776 - accuracy: 0.4313 - val_loss: 1.0466 - val_accuracy: 0.3884\n",
      "Epoch 3/3\n",
      "880/880 [==============================] - 991s 1s/step - loss: 0.9431 - accuracy: 0.4648 - val_loss: 1.0882 - val_accuracy: 0.3209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18ddad4be50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_padded, \n",
    "          y_train_one_hot, \n",
    "          epochs=3, \n",
    "          batch_size=32, \n",
    "          validation_data=(X_test_padded, y_test_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53ae7952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 55s 252ms/step - loss: 1.0882 - accuracy: 0.3209\n",
      "Test Loss: 1.0882054567337036\n",
      "Test Accuracy: 0.3208806812763214\n"
     ]
    }
   ],
   "source": [
    "evaluation_result = model.evaluate(X_test_padded, y_test_one_hot)\n",
    "\n",
    "# Print the evaluation result\n",
    "print(\"Test Loss:\", evaluation_result[0])\n",
    "print(\"Test Accuracy:\", evaluation_result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46b2c4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 268s 305ms/step\n",
      "220/220 [==============================] - 65s 296ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred_test_prob, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate performance metrics\u001b[39;00m\n\u001b[0;32m     10\u001b[0m performance_tf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError_metric\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     11\u001b[0m                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m: [accuracy_score(y_train, y_pred_train),\n\u001b[1;32m---> 12\u001b[0m                                        precision_score(y_train, y_pred_train, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     13\u001b[0m                                        recall_score(y_train, y_pred_train, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)],\n\u001b[0;32m     14\u001b[0m                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m: [accuracy_score(y_test, y_pred_test),\n\u001b[0;32m     15\u001b[0m                                       precision_score(y_test, y_pred_test, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     16\u001b[0m                                       recall_score(y_test, y_pred_test, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]})\n\u001b[0;32m     19\u001b[0m display(performance_tf)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1954\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprecision_score\u001b[39m(\n\u001b[0;32m   1826\u001b[0m     y_true,\n\u001b[0;32m   1827\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1833\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1834\u001b[0m ):\n\u001b[0;32m   1835\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   1836\u001b[0m \n\u001b[0;32m   1837\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1952\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   1953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1954\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   1955\u001b[0m         y_true,\n\u001b[0;32m   1956\u001b[0m         y_pred,\n\u001b[0;32m   1957\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1958\u001b[0m         pos_label\u001b[38;5;241m=\u001b[39mpos_label,\n\u001b[0;32m   1959\u001b[0m         average\u001b[38;5;241m=\u001b[39maverage,\n\u001b[0;32m   1960\u001b[0m         warn_for\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m,),\n\u001b[0;32m   1961\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1962\u001b[0m         zero_division\u001b[38;5;241m=\u001b[39mzero_division,\n\u001b[0;32m   1963\u001b[0m     )\n\u001b[0;32m   1964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[38;5;241m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1377\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1374\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m-> 1377\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:117\u001b[0m, in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Check that we don't mix string type with number type\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m ys_labels)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMix of label input types (string and number)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28msorted\u001b[39m(ys_labels))\n",
      "\u001b[1;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "# Make predictions with the model\n",
    "y_pred_train_prob = model.predict(X_train_padded)\n",
    "y_pred_test_prob = model.predict(X_test_padded)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred_train = np.argmax(y_pred_train_prob, axis=1)\n",
    "y_pred_test = np.argmax(y_pred_test_prob, axis=1)\n",
    "\n",
    "# Calculate performance metrics\n",
    "performance_tf = pd.DataFrame({'Error_metric': ['Accuracy', 'Precision', 'Recall'],\n",
    "                             'Train': [accuracy_score(y_train, y_pred_train),\n",
    "                                       precision_score(y_train, y_pred_train, average='weighted', zero_division=1),\n",
    "                                       recall_score(y_train, y_pred_train, average='weighted', zero_division=1)],\n",
    "                             'Test': [accuracy_score(y_test, y_pred_test),\n",
    "                                      precision_score(y_test, y_pred_test, average='weighted', zero_division=1),\n",
    "                                      recall_score(y_test, y_pred_test, average='weighted', zero_division=1)]})\n",
    "\n",
    "\n",
    "display(performance_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a83945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map class indices to class labels\n",
    "y_pred_train_labels = [class_labels[idx] for idx in y_pred_train]\n",
    "y_pred_test_labels = [class_labels[idx] for idx in y_pred_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06eab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in SavedModel format\n",
    "#model.save('tf3_model') ### for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2f83d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
